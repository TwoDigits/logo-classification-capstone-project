{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project of the Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Logo Detection App\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, we make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, our code will accept any user-supplied image as input.  If a logo is detected in the image, it will provide an estimate of the brand.  \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, we need to piece together a series of models to perform different tasks; for instance, the algorithm that detects logos in an image will be different from the CNN that infers the brand.  \n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Logos\n",
    "* [Step 3](#step2): Create a CNN to Classify Brands (from Scratch)\n",
    "* [Step 4](#step3): Use a CNN to Classify Brands (using Transfer Learning)\n",
    "* [Step 5](#step4): Create a CNN to Classify Brands (using Transfer Learning)\n",
    "* [Step 6](#step5): Write the Algorithm\n",
    "* [Step 7](#step6): Test the Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Logo in the Wild Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of logo images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `logo_names` - list of string-valued brand names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand names:  ['voc_format/FedEx', 'voc_format/budweiser', 'voc_format/aspirin', 'voc_format/azeca', 'voc_format/bello digital', 'voc_format/airhawk', 'voc_format/chanel', 'voc_format/caterpillar', 'voc_format/rolex', 'voc_format/toyota', 'voc_format/athalon', 'voc_format/Samsung', 'voc_format/aquapac', 'voc_format/verizon', 'voc_format/LOreal', 'voc_format/American Express', 'voc_format/BMW', 'voc_format/boeing', 'voc_format/sony', 'voc_format/santander', 'voc_format/McDonalds', 'voc_format/panasonic', 'voc_format/nescafe', 'voc_format/hershey', 'voc_format/gucci', 'voc_format/shell', 'voc_format/porsche', 'voc_format/colgate', 'voc_format/huawei', 'voc_format/chevrolet', 'voc_format/bionade', 'voc_format/nivea', 'voc_format/bosch', 'voc_format/costco', 'voc_format/kia', 'voc_format/honda', 'voc_format/uniqlo', 'voc_format/visa', 'voc_format/ford', 'voc_format/ben sherman', 'voc_format/burger king', 'voc_format/lego', 'voc_format/pizza hut', 'voc_format/bank of america', 'voc_format/frito lays', 'voc_format/amcrest', 'voc_format/netflix', 'voc_format/hp', 'voc_format/Pampers', 'voc_format/heineken', 'voc_format/wells fargo', 'voc_format/nestle', 'voc_format/nike', 'voc_format/abus', 'voc_format/hsbc', 'voc_format/starbucks', 'voc_format/kraft', 'voc_format/aral', 'voc_format/pepsi', 'voc_format/home depot', 'voc_format/allett', 'voc_format/bem wireless', 'voc_format/aluratek', 'voc_format/mercedes benz', 'voc_format/ape', 'voc_format/axa', 'voc_format/accenture', 'voc_format/subway', 'voc_format/allianz', 'voc_format/esso', 'voc_format/CVS', 'voc_format/IBM', 'voc_format/apc', 'voc_format/armitron', 'voc_format/lexus', 'voc_format/tmobile', 'voc_format/benrus', 'voc_format/danone', 'voc_format/canon', 'voc_format/philips', 'voc_format/UPS', 'voc_format/louis vuitton', 'voc_format/cisco', 'voc_format/cartier', 'voc_format/gillette', 'voc_format/amazon', 'voc_format/red bull', 'voc_format/hyundai', 'voc_format/coca-cola', 'voc_format/nissan', 'voc_format/bella taylor', 'voc_format/corona', 'voc_format/intel', 'voc_format/audi', 'voc_format/bertha watches', 'voc_format/reebok', 'voc_format/IKEA', 'voc_format/BASF', 'voc_format/kelloggs', 'voc_format/Walmart', 'voc_format/H&M', 'voc_format/aldi', 'voc_format/marlboro', 'voc_format/target', 'voc_format/Zara', 'voc_format/bellabeat', 'voc_format/adidas', 'voc_format/volkswagen', 'voc_format/mastercard']\n",
      "There are 109 total brand categories.\n",
      "There are 9428 JPEG images with logos.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_jpg_dataset(path):\n",
    "    data = load_files(path)\n",
    "    logo_files = np.array(data['filenames'])\n",
    "    indices_of_jpegs = [i for i, j in enumerate(logo_files) if '.jp' in j]\n",
    "    logo_targets = np_utils.to_categorical(np.array(data['target']), max(data['target']+1))\n",
    "    return logo_files[indices_of_jpegs], logo_targets[indices_of_jpegs]\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "all_files, all_targets = load_jpg_dataset('LogosInTheWild-v2/data/voc_format')\n",
    "#valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "#test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "brand_names = [item[23:-1] for item in (glob(\"LogosInTheWild-v2/data/voc_format/*/\"))]\n",
    "\n",
    "print(\"brand names: \",brand_names)\n",
    "# print statistics about the dataset\n",
    "print('There are %d total brand categories.' % len(brand_names))\n",
    "print('There are %d JPEG images with logos.' % len(all_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5656,)\n",
      "(5656, 109)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_files, test_files, train_targets, test_targets = train_test_split(all_files, all_targets, test_size=0.4, random_state=0)\n",
    "#train_and_val_files, test_files, train_and_val_targets, test_targets = train_test_split(all_files, all_targets, test_size=0.4, random_state=0)\n",
    "#train_files, val_files, train_targets, val_targets = train_test_split(train_and_val_files, train_and_val_targets, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "n_samples = train_files.shape[0]\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "print(train_files.shape)\n",
    "print(train_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Logos\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect logos in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Logos (from Scratch)\n",
    "\n",
    "Now we want a way to predict logo brands from images.  In this step, you will create a CNN that classifies logos.  \n",
    "\n",
    "We do not add too many trainable layers as more parameters mean longer training and we do not have a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take.\n",
    "\n",
    "We mention that the task of classifying small logos in images is considered exceptionally challenging.  \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5656/5656 [02:40<00:00, 32.10it/s]\n",
      "100%|██████████| 3772/3772 [01:57<00:00, 32.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "#val_tensors = paths_to_tensor(val_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 224, 224, 3)\n",
      "(None, 223, 223, 32)\n",
      "(None, 111, 111, 32)\n",
      "(None, 110, 110, 64)\n",
      "(None, 55, 55, 64)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 64)      8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 109)               7085      \n",
      "=================================================================\n",
      "Total params: 15,757\n",
      "Trainable params: 15,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D (kernel_size = (2,2), filters = 32, input_shape=train_tensors.shape[1:], activation='relu',data_format=\"channels_last\"))\n",
    "print(model.input_shape)\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "print(model.output_shape)\n",
    "model.add(Conv2D (kernel_size = 2, filters = 64, activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling2D(pool_size = 2, strides=2))\n",
    "print(model.output_shape)\n",
    "model.add(GlobalAveragePooling2D(data_format=None))\n",
    "model.add(Dense(109, activation = 'softmax'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3959 samples, validate on 1697 samples\n",
      "Epoch 1/20\n",
      "3959/3959 [==============================] - 204s 51ms/step - loss: 4.3102 - acc: 0.0543 - val_loss: 4.2993 - val_acc: 0.0707\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.29928, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/20\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 4.2462 - acc: 0.0674 - val_loss: 4.2489 - val_acc: 0.0825\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.29928 to 4.24890, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/20\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 4.1761 - acc: 0.0806 - val_loss: 4.1826 - val_acc: 0.0860\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.24890 to 4.18261, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/20\n",
      "3959/3959 [==============================] - 147s 37ms/step - loss: 4.1114 - acc: 0.0791 - val_loss: 4.1590 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.18261 to 4.15900, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/20\n",
      "3959/3959 [==============================] - 144s 36ms/step - loss: 4.0648 - acc: 0.0882 - val_loss: 4.0894 - val_acc: 0.1072\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.15900 to 4.08941, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/20\n",
      "3959/3959 [==============================] - 137s 34ms/step - loss: 4.0239 - acc: 0.0962 - val_loss: 4.0588 - val_acc: 0.1102\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.08941 to 4.05880, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/20\n",
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.9857 - acc: 0.1003 - val_loss: 4.0287 - val_acc: 0.1232\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.05880 to 4.02870, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/20\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.9525 - acc: 0.1084 - val_loss: 4.0327 - val_acc: 0.1019\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.02870\n",
      "Epoch 9/20\n",
      "3959/3959 [==============================] - 135s 34ms/step - loss: 3.9220 - acc: 0.1147 - val_loss: 3.9837 - val_acc: 0.1255\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.02870 to 3.98371, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/20\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.8912 - acc: 0.1132 - val_loss: 3.9712 - val_acc: 0.1243\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.98371 to 3.97120, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 11/20\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.8637 - acc: 0.1182 - val_loss: 3.9371 - val_acc: 0.1379\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.97120 to 3.93706, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 12/20\n",
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.8386 - acc: 0.1263 - val_loss: 3.9196 - val_acc: 0.1391\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.93706 to 3.91960, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 13/20\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.8163 - acc: 0.1223 - val_loss: 3.9139 - val_acc: 0.1450\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.91960 to 3.91389, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 14/20\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.7950 - acc: 0.1351 - val_loss: 3.8827 - val_acc: 0.1491\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.91389 to 3.88273, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 15/20\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.7792 - acc: 0.1324 - val_loss: 3.8688 - val_acc: 0.1532\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.88273 to 3.86877, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 16/20\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.7618 - acc: 0.1329 - val_loss: 3.8885 - val_acc: 0.1397\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.86877\n",
      "Epoch 17/20\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.7457 - acc: 0.1394 - val_loss: 3.8472 - val_acc: 0.1544\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.86877 to 3.84725, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 18/20\n",
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.7279 - acc: 0.1422 - val_loss: 3.8592 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.84725\n",
      "Epoch 19/20\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.7202 - acc: 0.1420 - val_loss: 3.8261 - val_acc: 0.1615\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.84725 to 3.82612, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 20/20\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.7052 - acc: 0.1450 - val_loss: 3.8395 - val_acc: 0.1544\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.82612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1aed3fbe0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_keras_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets,  validation_split=0.3,\n",
    "          #validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3959 samples, validate on 1697 samples\n",
      "Epoch 1/100\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.7062 - acc: 0.1437 - val_loss: 3.8193 - val_acc: 0.1656\n",
      "\n",
      "Epoch 00001: val_loss improved from 3.82612 to 3.81926, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/100\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.6942 - acc: 0.1526 - val_loss: 3.8133 - val_acc: 0.1638\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.81926 to 3.81326, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/100\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.6804 - acc: 0.1528 - val_loss: 3.8276 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.81326\n",
      "Epoch 4/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.6683 - acc: 0.1518 - val_loss: 3.7873 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.81326 to 3.78730, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.6560 - acc: 0.1533 - val_loss: 3.7970 - val_acc: 0.1668\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.78730\n",
      "Epoch 6/100\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.6466 - acc: 0.1566 - val_loss: 3.7914 - val_acc: 0.1621\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.78730\n",
      "Epoch 7/100\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.6344 - acc: 0.1564 - val_loss: 3.7815 - val_acc: 0.1597\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.78730 to 3.78148, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/100\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.6244 - acc: 0.1614 - val_loss: 3.7617 - val_acc: 0.1679\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.78148 to 3.76171, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 9/100\n",
      "3959/3959 [==============================] - 143s 36ms/step - loss: 3.6158 - acc: 0.1642 - val_loss: 3.7604 - val_acc: 0.1679\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.76171 to 3.76036, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.6049 - acc: 0.1614 - val_loss: 3.7512 - val_acc: 0.1715\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.76036 to 3.75121, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 11/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.5965 - acc: 0.1657 - val_loss: 3.7844 - val_acc: 0.1768\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.75121\n",
      "Epoch 12/100\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.5879 - acc: 0.1644 - val_loss: 3.7521 - val_acc: 0.1668\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.75121\n",
      "Epoch 13/100\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.5787 - acc: 0.1695 - val_loss: 3.7360 - val_acc: 0.1768\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.75121 to 3.73600, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 14/100\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.5725 - acc: 0.1725 - val_loss: 3.7351 - val_acc: 0.1750\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.73600 to 3.73509, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 15/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.5582 - acc: 0.1758 - val_loss: 3.7208 - val_acc: 0.1874\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.73509 to 3.72080, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 16/100\n",
      "3959/3959 [==============================] - 141s 35ms/step - loss: 3.5515 - acc: 0.1745 - val_loss: 3.7262 - val_acc: 0.1780\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.72080\n",
      "Epoch 17/100\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.5452 - acc: 0.1786 - val_loss: 3.7142 - val_acc: 0.1856\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.72080 to 3.71421, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 18/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.5368 - acc: 0.1851 - val_loss: 3.7173 - val_acc: 0.1862\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.71421\n",
      "Epoch 19/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.5263 - acc: 0.1826 - val_loss: 3.7024 - val_acc: 0.1850\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.71421 to 3.70236, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 20/100\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.5169 - acc: 0.1849 - val_loss: 3.7100 - val_acc: 0.1886\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.70236\n",
      "Epoch 21/100\n",
      "3959/3959 [==============================] - 140s 35ms/step - loss: 3.5118 - acc: 0.1826 - val_loss: 3.7160 - val_acc: 0.1933\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3.70236\n",
      "Epoch 22/100\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.5013 - acc: 0.1836 - val_loss: 3.7004 - val_acc: 0.1827\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.70236 to 3.70043, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 23/100\n",
      "3959/3959 [==============================] - 145s 37ms/step - loss: 3.4948 - acc: 0.1879 - val_loss: 3.6957 - val_acc: 0.1839\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.70043 to 3.69574, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 24/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.4862 - acc: 0.1862 - val_loss: 3.6875 - val_acc: 0.1827\n",
      "\n",
      "Epoch 00024: val_loss improved from 3.69574 to 3.68754, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 25/100\n",
      "3959/3959 [==============================] - 150s 38ms/step - loss: 3.4765 - acc: 0.1894 - val_loss: 3.6821 - val_acc: 0.1909\n",
      "\n",
      "Epoch 00025: val_loss improved from 3.68754 to 3.68206, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 26/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4682 - acc: 0.1942 - val_loss: 3.6954 - val_acc: 0.1909\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 3.68206\n",
      "Epoch 27/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4626 - acc: 0.1917 - val_loss: 3.6850 - val_acc: 0.1874\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 3.68206\n",
      "Epoch 28/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4559 - acc: 0.1963 - val_loss: 3.6760 - val_acc: 0.1951\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.68206 to 3.67602, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 29/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4474 - acc: 0.1953 - val_loss: 3.6954 - val_acc: 0.1897\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.67602\n",
      "Epoch 30/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4416 - acc: 0.1965 - val_loss: 3.6519 - val_acc: 0.1897\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.67602 to 3.65189, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 31/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4329 - acc: 0.1980 - val_loss: 3.6815 - val_acc: 0.1909\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.65189\n",
      "Epoch 32/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4268 - acc: 0.1950 - val_loss: 3.6638 - val_acc: 0.1868\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.65189\n",
      "Epoch 33/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4166 - acc: 0.2061 - val_loss: 3.6579 - val_acc: 0.1903\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 3.65189\n",
      "Epoch 34/100\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.4092 - acc: 0.2008 - val_loss: 3.6592 - val_acc: 0.1939\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 3.65189\n",
      "Epoch 35/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.4029 - acc: 0.2036 - val_loss: 3.6530 - val_acc: 0.1945\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.65189\n",
      "Epoch 36/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.3952 - acc: 0.2061 - val_loss: 3.6385 - val_acc: 0.1998\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.65189 to 3.63846, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.3939 - acc: 0.2051 - val_loss: 3.6296 - val_acc: 0.1980\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.63846 to 3.62965, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 38/100\n",
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.3851 - acc: 0.2033 - val_loss: 3.6228 - val_acc: 0.2033\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.62965 to 3.62283, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 39/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3775 - acc: 0.2089 - val_loss: 3.6292 - val_acc: 0.1992\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.62283\n",
      "Epoch 40/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3674 - acc: 0.2167 - val_loss: 3.6130 - val_acc: 0.2039\n",
      "\n",
      "Epoch 00040: val_loss improved from 3.62283 to 3.61297, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 41/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3606 - acc: 0.2099 - val_loss: 3.6239 - val_acc: 0.2057\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 3.61297\n",
      "Epoch 42/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3558 - acc: 0.2074 - val_loss: 3.6165 - val_acc: 0.2133\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 3.61297\n",
      "Epoch 43/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3452 - acc: 0.2107 - val_loss: 3.6134 - val_acc: 0.2080\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.61297\n",
      "Epoch 44/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3416 - acc: 0.2177 - val_loss: 3.6047 - val_acc: 0.2027\n",
      "\n",
      "Epoch 00044: val_loss improved from 3.61297 to 3.60475, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 45/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3341 - acc: 0.2137 - val_loss: 3.5953 - val_acc: 0.2021\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.60475 to 3.59527, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 46/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3307 - acc: 0.2170 - val_loss: 3.6149 - val_acc: 0.2021\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 3.59527\n",
      "Epoch 47/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3245 - acc: 0.2172 - val_loss: 3.6184 - val_acc: 0.2068\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 3.59527\n",
      "Epoch 48/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3168 - acc: 0.2192 - val_loss: 3.5852 - val_acc: 0.2039\n",
      "\n",
      "Epoch 00048: val_loss improved from 3.59527 to 3.58517, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 49/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3093 - acc: 0.2203 - val_loss: 3.5983 - val_acc: 0.2062\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 3.58517\n",
      "Epoch 50/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3023 - acc: 0.2170 - val_loss: 3.6005 - val_acc: 0.2092\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.58517\n",
      "Epoch 51/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.3019 - acc: 0.2233 - val_loss: 3.6241 - val_acc: 0.2098\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 3.58517\n",
      "Epoch 52/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2920 - acc: 0.2220 - val_loss: 3.5783 - val_acc: 0.2098\n",
      "\n",
      "Epoch 00052: val_loss improved from 3.58517 to 3.57834, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 53/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2863 - acc: 0.2248 - val_loss: 3.6125 - val_acc: 0.2086\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 3.57834\n",
      "Epoch 54/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2829 - acc: 0.2266 - val_loss: 3.5918 - val_acc: 0.1986\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 3.57834\n",
      "Epoch 55/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2750 - acc: 0.2238 - val_loss: 3.5515 - val_acc: 0.2098\n",
      "\n",
      "Epoch 00055: val_loss improved from 3.57834 to 3.55151, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 56/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2722 - acc: 0.2253 - val_loss: 3.5889 - val_acc: 0.2092\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 3.55151\n",
      "Epoch 57/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2680 - acc: 0.2170 - val_loss: 3.6062 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 3.55151\n",
      "Epoch 58/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2590 - acc: 0.2253 - val_loss: 3.5626 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 3.55151\n",
      "Epoch 59/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2556 - acc: 0.2276 - val_loss: 3.5528 - val_acc: 0.2051\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 3.55151\n",
      "Epoch 60/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2539 - acc: 0.2321 - val_loss: 3.5816 - val_acc: 0.2086\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 3.55151\n",
      "Epoch 61/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2446 - acc: 0.2321 - val_loss: 3.5402 - val_acc: 0.2145\n",
      "\n",
      "Epoch 00061: val_loss improved from 3.55151 to 3.54023, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 62/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2403 - acc: 0.2314 - val_loss: 3.5478 - val_acc: 0.2110\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 3.54023\n",
      "Epoch 63/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2293 - acc: 0.2306 - val_loss: 3.5521 - val_acc: 0.2115\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 3.54023\n",
      "Epoch 64/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2311 - acc: 0.2326 - val_loss: 3.5422 - val_acc: 0.2110\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 3.54023\n",
      "Epoch 65/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2238 - acc: 0.2286 - val_loss: 3.5444 - val_acc: 0.2062\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 3.54023\n",
      "Epoch 66/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2219 - acc: 0.2291 - val_loss: 3.5813 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 3.54023\n",
      "Epoch 67/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2157 - acc: 0.2412 - val_loss: 3.5911 - val_acc: 0.2027\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 3.54023\n",
      "Epoch 68/100\n",
      "3959/3959 [==============================] - 135s 34ms/step - loss: 3.2151 - acc: 0.2342 - val_loss: 3.5409 - val_acc: 0.2145\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 3.54023\n",
      "Epoch 69/100\n",
      "3959/3959 [==============================] - 135s 34ms/step - loss: 3.2053 - acc: 0.2344 - val_loss: 3.5315 - val_acc: 0.2174\n",
      "\n",
      "Epoch 00069: val_loss improved from 3.54023 to 3.53153, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 70/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.2005 - acc: 0.2382 - val_loss: 3.5594 - val_acc: 0.2057\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 3.53153\n",
      "Epoch 71/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1985 - acc: 0.2395 - val_loss: 3.5905 - val_acc: 0.2039\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 3.53153\n",
      "Epoch 72/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1940 - acc: 0.2392 - val_loss: 3.5307 - val_acc: 0.2139\n",
      "\n",
      "Epoch 00072: val_loss improved from 3.53153 to 3.53069, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 73/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1899 - acc: 0.2377 - val_loss: 3.5221 - val_acc: 0.2127\n",
      "\n",
      "Epoch 00073: val_loss improved from 3.53069 to 3.52210, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 74/100\n",
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.1834 - acc: 0.2468 - val_loss: 3.5128 - val_acc: 0.2110\n",
      "\n",
      "Epoch 00074: val_loss improved from 3.52210 to 3.51280, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 75/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1763 - acc: 0.2389 - val_loss: 3.5281 - val_acc: 0.2180\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 3.51280\n",
      "Epoch 76/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1729 - acc: 0.2463 - val_loss: 3.5117 - val_acc: 0.2186\n",
      "\n",
      "Epoch 00076: val_loss improved from 3.51280 to 3.51167, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 77/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1706 - acc: 0.2427 - val_loss: 3.5177 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 3.51167\n",
      "Epoch 78/100\n",
      "3959/3959 [==============================] - 136s 34ms/step - loss: 3.1627 - acc: 0.2450 - val_loss: 3.5546 - val_acc: 0.2180\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 3.51167\n",
      "Epoch 79/100\n",
      "3959/3959 [==============================] - 137s 35ms/step - loss: 3.1641 - acc: 0.2445 - val_loss: 3.5747 - val_acc: 0.2127\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 3.51167\n",
      "Epoch 80/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.1577 - acc: 0.2465 - val_loss: 3.5296 - val_acc: 0.2174\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 3.51167\n",
      "Epoch 81/100\n",
      "3959/3959 [==============================] - 147s 37ms/step - loss: 3.1480 - acc: 0.2526 - val_loss: 3.5701 - val_acc: 0.2045\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 3.51167\n",
      "Epoch 82/100\n",
      "3959/3959 [==============================] - 161s 41ms/step - loss: 3.1440 - acc: 0.2539 - val_loss: 3.5174 - val_acc: 0.2127\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 3.51167\n",
      "Epoch 83/100\n",
      "3959/3959 [==============================] - 153s 39ms/step - loss: 3.1425 - acc: 0.2488 - val_loss: 3.5503 - val_acc: 0.2145\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 3.51167\n",
      "Epoch 84/100\n",
      "3959/3959 [==============================] - 146s 37ms/step - loss: 3.1413 - acc: 0.2541 - val_loss: 3.5228 - val_acc: 0.2163\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 3.51167\n",
      "Epoch 85/100\n",
      "3959/3959 [==============================] - 143s 36ms/step - loss: 3.1387 - acc: 0.2546 - val_loss: 3.5481 - val_acc: 0.2080\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 3.51167\n",
      "Epoch 86/100\n",
      "3959/3959 [==============================] - 151s 38ms/step - loss: 3.1293 - acc: 0.2546 - val_loss: 3.5556 - val_acc: 0.2104\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 3.51167\n",
      "Epoch 87/100\n",
      "3959/3959 [==============================] - 144s 36ms/step - loss: 3.1297 - acc: 0.2544 - val_loss: 3.5183 - val_acc: 0.2192\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 3.51167\n",
      "Epoch 88/100\n",
      "3959/3959 [==============================] - 145s 37ms/step - loss: 3.1232 - acc: 0.2539 - val_loss: 3.5581 - val_acc: 0.2086\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 3.51167\n",
      "Epoch 89/100\n",
      "3959/3959 [==============================] - 142s 36ms/step - loss: 3.1172 - acc: 0.2622 - val_loss: 3.5516 - val_acc: 0.2216\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 3.51167\n",
      "Epoch 90/100\n",
      "3959/3959 [==============================] - 141s 36ms/step - loss: 3.1175 - acc: 0.2614 - val_loss: 3.4922 - val_acc: 0.2192\n",
      "\n",
      "Epoch 00090: val_loss improved from 3.51167 to 3.49223, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 91/100\n",
      "3959/3959 [==============================] - 144s 36ms/step - loss: 3.1086 - acc: 0.2574 - val_loss: 3.4984 - val_acc: 0.2227\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 3.49223\n",
      "Epoch 92/100\n",
      "3959/3959 [==============================] - 147s 37ms/step - loss: 3.1063 - acc: 0.2503 - val_loss: 3.4988 - val_acc: 0.2263\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 3.49223\n",
      "Epoch 93/100\n",
      "3959/3959 [==============================] - 145s 37ms/step - loss: 3.1019 - acc: 0.2660 - val_loss: 3.4984 - val_acc: 0.2204\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 3.49223\n",
      "Epoch 94/100\n",
      "3959/3959 [==============================] - 154s 39ms/step - loss: 3.0960 - acc: 0.2599 - val_loss: 3.5101 - val_acc: 0.2239\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 3.49223\n",
      "Epoch 95/100\n",
      "3959/3959 [==============================] - 152s 39ms/step - loss: 3.1012 - acc: 0.2584 - val_loss: 3.4885 - val_acc: 0.2174\n",
      "\n",
      "Epoch 00095: val_loss improved from 3.49223 to 3.48850, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 96/100\n",
      "3959/3959 [==============================] - 150s 38ms/step - loss: 3.0887 - acc: 0.2650 - val_loss: 3.5309 - val_acc: 0.2204\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 3.48850\n",
      "Epoch 97/100\n",
      "3959/3959 [==============================] - 154s 39ms/step - loss: 3.0902 - acc: 0.2690 - val_loss: 3.4871 - val_acc: 0.2239\n",
      "\n",
      "Epoch 00097: val_loss improved from 3.48850 to 3.48706, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 98/100\n",
      "3959/3959 [==============================] - 139s 35ms/step - loss: 3.0863 - acc: 0.2650 - val_loss: 3.4962 - val_acc: 0.2192\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 3.48706\n",
      "Epoch 99/100\n",
      "3959/3959 [==============================] - 144s 36ms/step - loss: 3.0772 - acc: 0.2612 - val_loss: 3.4527 - val_acc: 0.2239\n",
      "\n",
      "Epoch 00099: val_loss improved from 3.48706 to 3.45273, saving model to saved_keras_models/weights.best.from_scratch.hdf5\n",
      "Epoch 100/100\n",
      "3959/3959 [==============================] - 138s 35ms/step - loss: 3.0773 - acc: 0.2672 - val_loss: 3.4922 - val_acc: 0.2257\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 3.45273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1aed5d208>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "#checkpointer2 = ModelCheckpoint(filepath='saved_keras_models/weights-Copy1.best.from_scratch.hdf5', \n",
    "#verbose=1, save_best_only=True)\n",
    "model.load_weights('saved_keras_models/weights.best.from_scratch.hdf5')\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_split=0.3,\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_keras_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 22.9852%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted logo brand for each image in test set\n",
    "brand_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(brand_predictions)==np.argmax(test_targets, axis=1))/len(brand_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Logos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_tensors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ee82f4eb1fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mVGG16_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVGG16_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_tensors' is not defined"
     ]
    }
   ],
   "source": [
    "#VGG16_model = Sequential()\n",
    "#VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "#VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "#VGG16_model.summary()\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Conv2D,MaxPooling2D, Flatten,Dense,Dropout,GlobalAveragePooling2D\n",
    "VGG16_model = VGG16(include_top=True, weights='imagenet', input_shape=train_tensors.shape[1:])\n",
    "print(VGG16_model.summary())\n",
    "\n",
    "# Creating dictionary that maps layer names to the layers\n",
    "layer_dict = dict([(layer.name, layer) for layer in VGG16_model.layers])\n",
    "\n",
    "# Getting output tensor of the last VGG layer that we want to include\n",
    "x = layer_dict['flatten'].output\n",
    "\n",
    "# Stacking a new simple convolutional network on top of it    \n",
    "x=GlobalAveragePooling2D(data_format=None)(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = GlobalAveragePooling2D(256)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(109, activation='softmax')(x)\n",
    "\n",
    "# Creating new model. Please note that this is NOT a Sequential() model.\n",
    "from keras.models import Model\n",
    "custom_model = Model(input=VGG16_model.input, output=x)\n",
    "\n",
    "# Make sure that the pre-trained bottom layers are not trainable\n",
    "for layer in custom_model.layers[:7]:\n",
    "    layer.trainable = True\n",
    "\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_model.load_weights('saved_models/weights.best.VGG16.hdf5')\n",
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "custom_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3889 samples, validate on 1667 samples\n",
      "Epoch 1/10\n",
      "3760/3889 [============================>.] - ETA: 2:33:16 - loss: 5.1215 - acc: 0.0000e+ - ETA: 2:26:33 - loss: 10.6198 - acc: 0.0000e+0 - ETA: 2:40:51 - loss: 12.4526 - acc: 0.0000e+0 - ETA: 2:37:19 - loss: 13.3690 - acc: 0.0000e+0 - ETA: 2:30:34 - loss: 13.9188 - acc: 0.0000e+0 - ETA: 2:27:32 - loss: 14.2853 - acc: 0.0000e+0 - ETA: 2:25:02 - loss: 14.5472 - acc: 0.0000e+0 - ETA: 2:20:04 - loss: 14.7435 - acc: 0.0000e+0 - ETA: 2:16:37 - loss: 14.8963 - acc: 0.0000e+0 - ETA: 2:12:38 - loss: 15.0184 - acc: 0.0000e+0 - ETA: 2:11:21 - loss: 15.1184 - acc: 0.0000e+0 - ETA: 2:09:24 - loss: 15.2017 - acc: 0.0000e+0 - ETA: 2:07:27 - loss: 15.2722 - acc: 0.0000e+0 - ETA: 2:05:48 - loss: 15.3326 - acc: 0.0000e+0 - ETA: 2:03:27 - loss: 15.3850 - acc: 0.0000e+0 - ETA: 2:01:12 - loss: 15.4308 - acc: 0.0000e+0 - ETA: 1:58:59 - loss: 15.4712 - acc: 0.0000e+0 - ETA: 1:56:45 - loss: 15.5072 - acc: 0.0000e+0 - ETA: 1:54:43 - loss: 15.5393 - acc: 0.0000e+0 - ETA: 1:52:48 - loss: 15.5683 - acc: 0.0000e+0 - ETA: 1:50:56 - loss: 15.5944 - acc: 0.0000e+0 - ETA: 1:49:19 - loss: 15.5816 - acc: 0.0023    - ETA: 1:47:43 - loss: 15.6049 - acc: 0.002 - ETA: 1:46:20 - loss: 15.6263 - acc: 0.002 - ETA: 1:44:57 - loss: 15.6460 - acc: 0.002 - ETA: 1:43:40 - loss: 15.6642 - acc: 0.001 - ETA: 1:42:22 - loss: 15.6810 - acc: 0.001 - ETA: 1:41:08 - loss: 15.6966 - acc: 0.001 - ETA: 1:39:55 - loss: 15.7111 - acc: 0.001 - ETA: 1:38:47 - loss: 15.7247 - acc: 0.001 - ETA: 1:37:44 - loss: 15.7374 - acc: 0.001 - ETA: 1:36:39 - loss: 15.7493 - acc: 0.001 - ETA: 1:35:38 - loss: 15.7604 - acc: 0.001 - ETA: 1:34:40 - loss: 15.7473 - acc: 0.002 - ETA: 1:33:45 - loss: 15.7579 - acc: 0.002 - ETA: 1:32:49 - loss: 15.7455 - acc: 0.004 - ETA: 1:31:59 - loss: 15.7555 - acc: 0.004 - ETA: 1:31:10 - loss: 15.7651 - acc: 0.003 - ETA: 1:30:21 - loss: 15.7741 - acc: 0.003 - ETA: 1:29:28 - loss: 15.7827 - acc: 0.003 - ETA: 1:28:34 - loss: 15.7909 - acc: 0.003 - ETA: 1:27:46 - loss: 15.7987 - acc: 0.003 - ETA: 1:26:54 - loss: 15.8061 - acc: 0.003 - ETA: 1:26:07 - loss: 15.8132 - acc: 0.003 - ETA: 1:25:20 - loss: 15.8200 - acc: 0.003 - ETA: 1:24:35 - loss: 15.8090 - acc: 0.004 - ETA: 1:23:48 - loss: 15.8155 - acc: 0.004 - ETA: 1:23:04 - loss: 15.8218 - acc: 0.004 - ETA: 1:22:18 - loss: 15.8279 - acc: 0.004 - ETA: 1:21:32 - loss: 15.8337 - acc: 0.004 - ETA: 1:20:51 - loss: 15.8393 - acc: 0.003 - ETA: 1:20:10 - loss: 15.8291 - acc: 0.004 - ETA: 1:19:27 - loss: 15.8346 - acc: 0.004 - ETA: 1:18:46 - loss: 15.8398 - acc: 0.004 - ETA: 1:18:04 - loss: 15.8449 - acc: 0.004 - ETA: 1:17:26 - loss: 15.8498 - acc: 0.004 - ETA: 1:16:48 - loss: 15.8545 - acc: 0.004 - ETA: 1:16:07 - loss: 15.8590 - acc: 0.004 - ETA: 1:15:27 - loss: 15.8634 - acc: 0.004 - ETA: 1:14:46 - loss: 15.8677 - acc: 0.004 - ETA: 1:14:05 - loss: 15.8718 - acc: 0.004 - ETA: 1:13:26 - loss: 15.8757 - acc: 0.004 - ETA: 1:12:46 - loss: 15.8668 - acc: 0.004 - ETA: 1:12:07 - loss: 15.8707 - acc: 0.004 - ETA: 1:11:27 - loss: 15.8745 - acc: 0.004 - ETA: 1:10:49 - loss: 15.8782 - acc: 0.004 - ETA: 1:10:10 - loss: 15.8818 - acc: 0.004 - ETA: 1:09:32 - loss: 15.8853 - acc: 0.004 - ETA: 1:08:53 - loss: 15.8886 - acc: 0.004 - ETA: 1:08:14 - loss: 15.8919 - acc: 0.004 - ETA: 1:07:37 - loss: 15.8951 - acc: 0.004 - ETA: 1:06:59 - loss: 15.8870 - acc: 0.004 - ETA: 1:06:21 - loss: 15.8902 - acc: 0.004 - ETA: 1:05:45 - loss: 15.8933 - acc: 0.004 - ETA: 1:05:09 - loss: 15.8748 - acc: 0.006 - ETA: 1:04:32 - loss: 15.8780 - acc: 0.005 - ETA: 1:03:57 - loss: 15.8811 - acc: 0.005 - ETA: 1:03:20 - loss: 15.8841 - acc: 0.005 - ETA: 1:02:43 - loss: 15.8871 - acc: 0.005 - ETA: 1:02:08 - loss: 15.8900 - acc: 0.005 - ETA: 1:01:31 - loss: 15.8928 - acc: 0.005 - ETA: 1:00:55 - loss: 15.8955 - acc: 0.005 - ETA: 1:00:19 - loss: 15.8982 - acc: 0.005 - ETA: 59:44 - loss: 15.9008 - acc: 0.0054  - ETA: 59:10 - loss: 15.9034 - acc: 0.005 - ETA: 58:37 - loss: 15.9059 - acc: 0.005 - ETA: 58:03 - loss: 15.9083 - acc: 0.005 - ETA: 57:27 - loss: 15.9107 - acc: 0.005 - ETA: 56:52 - loss: 15.9130 - acc: 0.005 - ETA: 56:19 - loss: 15.9153 - acc: 0.005 - ETA: 55:45 - loss: 15.9175 - acc: 0.004 - ETA: 55:10 - loss: 15.9110 - acc: 0.005 - ETA: 54:37 - loss: 15.9045 - acc: 0.005 - ETA: 54:02 - loss: 15.9068 - acc: 0.005 - ETA: 53:28 - loss: 15.9090 - acc: 0.005 - ETA: 52:56 - loss: 15.9028 - acc: 0.006 - ETA: 52:23 - loss: 15.9050 - acc: 0.006 - ETA: 51:51 - loss: 15.9072 - acc: 0.006 - ETA: 51:17 - loss: 15.9093 - acc: 0.006 - ETA: 50:43 - loss: 15.9034 - acc: 0.006 - ETA: 50:12 - loss: 15.8975 - acc: 0.006 - ETA: 49:42 - loss: 15.8997 - acc: 0.006 - ETA: 49:13 - loss: 15.9018 - acc: 0.006 - ETA: 48:41 - loss: 15.9039 - acc: 0.006 - ETA: 48:11 - loss: 15.9059 - acc: 0.006 - ETA: 47:37 - loss: 15.9079 - acc: 0.006 - ETA: 47:02 - loss: 15.9099 - acc: 0.006 - ETA: 46:29 - loss: 15.9118 - acc: 0.006 - ETA: 45:56 - loss: 15.9063 - acc: 0.006 - ETA: 45:26 - loss: 15.9009 - acc: 0.007 - ETA: 44:55 - loss: 15.9029 - acc: 0.007 - ETA: 44:24 - loss: 15.9048 - acc: 0.007 - ETA: 43:53 - loss: 15.9067 - acc: 0.007 - ETA: 43:21 - loss: 15.9085 - acc: 0.007 - ETA: 42:49 - loss: 15.9033 - acc: 0.007 - ETA: 42:15 - loss: 15.9052 - acc: 0.007 - ETA: 41:44 - loss: 15.9070 - acc: 0.007 - ETA: 41:12 - loss: 15.9020 - acc: 0.007 - ETA: 40:39 - loss: 15.9038 - acc: 0.007 - ETA: 40:08 - loss: 15.9056 - acc: 0.007 - ETA: 39:37 - loss: 15.9007 - acc: 0.007 - ETA: 39:07 - loss: 15.9024 - acc: 0.007 - ETA: 38:35 - loss: 15.9042 - acc: 0.007 - ETA: 38:04 - loss: 15.8994 - acc: 0.008 - ETA: 37:33 - loss: 15.9012 - acc: 0.008 - ETA: 37:02 - loss: 15.9029 - acc: 0.007 - ETA: 36:30 - loss: 15.9046 - acc: 0.007 - ETA: 35:58 - loss: 15.9063 - acc: 0.007 - ETA: 35:25 - loss: 15.9079 - acc: 0.007 - ETA: 34:52 - loss: 15.9033 - acc: 0.008 - ETA: 34:19 - loss: 15.9050 - acc: 0.008 - ETA: 33:48 - loss: 15.9066 - acc: 0.008 - ETA: 33:15 - loss: 15.9021 - acc: 0.008 - ETA: 32:43 - loss: 15.9037 - acc: 0.008 - ETA: 32:11 - loss: 15.9053 - acc: 0.008 - ETA: 31:39 - loss: 15.9069 - acc: 0.008 - ETA: 31:06 - loss: 15.9084 - acc: 0.008 - ETA: 30:34 - loss: 15.9099 - acc: 0.008 - ETA: 30:01 - loss: 15.9114 - acc: 0.007 - ETA: 29:29 - loss: 15.9129 - acc: 0.007 - ETA: 28:56 - loss: 15.9144 - acc: 0.007 - ETA: 28:23 - loss: 15.9158 - acc: 0.007 - ETA: 27:50 - loss: 15.9172 - acc: 0.007 - ETA: 27:17 - loss: 15.9130 - acc: 0.008 - ETA: 26:43 - loss: 15.9089 - acc: 0.008 - ETA: 26:10 - loss: 15.9103 - acc: 0.008 - ETA: 25:38 - loss: 15.9117 - acc: 0.008 - ETA: 25:05 - loss: 15.9131 - acc: 0.008 - ETA: 24:33 - loss: 15.9145 - acc: 0.008 - ETA: 24:00 - loss: 15.9158 - acc: 0.008 - ETA: 23:28 - loss: 15.9172 - acc: 0.007 - ETA: 22:55 - loss: 15.9185 - acc: 0.007 - ETA: 22:23 - loss: 15.9145 - acc: 0.008 - ETA: 21:50 - loss: 15.9159 - acc: 0.008 - ETA: 21:17 - loss: 15.9172 - acc: 0.008 - ETA: 20:44 - loss: 15.9185 - acc: 0.008 - ETA: 20:11 - loss: 15.9197 - acc: 0.008 - ETA: 19:38 - loss: 15.9210 - acc: 0.007 - ETA: 19:05 - loss: 15.9222 - acc: 0.007 - ETA: 18:32 - loss: 15.9234 - acc: 0.007 - ETA: 18:00 - loss: 15.9247 - acc: 0.007 - ETA: 17:27 - loss: 15.9258 - acc: 0.007 - ETA: 16:54 - loss: 15.9270 - acc: 0.007 - ETA: 16:21 - loss: 15.9282 - acc: 0.007 - ETA: 15:49 - loss: 15.9293 - acc: 0.007 - ETA: 15:16 - loss: 15.9305 - acc: 0.007 - ETA: 14:44 - loss: 15.9316 - acc: 0.007 - ETA: 14:11 - loss: 15.9327 - acc: 0.007 - ETA: 13:38 - loss: 15.9338 - acc: 0.007 - ETA: 13:06 - loss: 15.9349 - acc: 0.007 - ETA: 12:34 - loss: 15.9360 - acc: 0.007 - ETA: 12:01 - loss: 15.9370 - acc: 0.007 - ETA: 11:29 - loss: 15.9381 - acc: 0.007 - ETA: 10:57 - loss: 15.9391 - acc: 0.007 - ETA: 10:24 - loss: 15.9401 - acc: 0.007 - ETA: 9:52 - loss: 15.9411 - acc: 0.007 - ETA: 9:20 - loss: 15.9421 - acc: 0.00 - ETA: 8:47 - loss: 15.9431 - acc: 0.00 - ETA: 8:15 - loss: 15.9441 - acc: 0.00 - ETA: 7:43 - loss: 15.9451 - acc: 0.00 - ETA: 7:11 - loss: 15.9460 - acc: 0.00 - ETA: 6:39 - loss: 15.9470 - acc: 0.00 - ETA: 6:07 - loss: 15.9479 - acc: 0.00 - ETA: 5:34 - loss: 15.9488 - acc: 0.00 - ETA: 5:02 - loss: 15.9497 - acc: 0.00 - ETA: 4:30 - loss: 15.9507 - acc: 0.00 - ETA: 3:58 - loss: 15.9472 - acc: 0.00 - ETA: 3:26 - loss: 15.9481 - acc: 0.003889/3889 [==============================] - ETA: 2:54 - loss: 15.9490 - acc: 0.00 - ETA: 2:22 - loss: 15.9499 - acc: 0.00 - ETA: 1:50 - loss: 15.9508 - acc: 0.00 - ETA: 1:18 - loss: 15.9517 - acc: 0.00 - ETA: 46s - loss: 15.9526 - acc: 0.0067 - ETA: 14s - loss: 15.9534 - acc: 0.006 - 7015s 2s/step - loss: 15.9538 - acc: 0.0067 - val_loss: 16.0504 - val_acc: 0.0042\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 16.05041, saving model to saved_models/weights.best.custom.hdf5\n",
      "Epoch 2/10\n",
      "3060/3889 [======================>.......] - ETA: 1:35:57 - loss: 16.1181 - acc: 0.0000e+0 - ETA: 1:36:21 - loss: 15.7151 - acc: 0.0250    - ETA: 1:35:33 - loss: 15.8495 - acc: 0.016 - ETA: 1:35:31 - loss: 15.9166 - acc: 0.012 - ETA: 1:34:40 - loss: 15.9569 - acc: 0.010 - ETA: 1:34:32 - loss: 15.9838 - acc: 0.008 - ETA: 1:33:43 - loss: 15.8878 - acc: 0.014 - ETA: 1:33:28 - loss: 15.9166 - acc: 0.012 - ETA: 1:32:52 - loss: 15.9390 - acc: 0.011 - ETA: 1:32:20 - loss: 15.9569 - acc: 0.010 - ETA: 1:31:49 - loss: 15.9716 - acc: 0.009 - ETA: 1:31:28 - loss: 15.9166 - acc: 0.012 - ETA: 1:31:05 - loss: 15.9321 - acc: 0.011 - ETA: 1:30:27 - loss: 15.9454 - acc: 0.010 - ETA: 1:29:55 - loss: 15.9032 - acc: 0.013 - ETA: 1:29:22 - loss: 15.9166 - acc: 0.012 - ETA: 1:28:53 - loss: 15.9285 - acc: 0.011 - ETA: 1:28:27 - loss: 15.9390 - acc: 0.011 - ETA: 1:27:53 - loss: 15.9484 - acc: 0.010 - ETA: 1:27:25 - loss: 15.9569 - acc: 0.010 - ETA: 1:26:52 - loss: 15.9646 - acc: 0.009 - ETA: 1:26:26 - loss: 15.9716 - acc: 0.009 - ETA: 1:25:50 - loss: 15.9779 - acc: 0.008 - ETA: 1:25:19 - loss: 15.9838 - acc: 0.008 - ETA: 1:24:45 - loss: 15.9892 - acc: 0.008 - ETA: 1:24:17 - loss: 15.9941 - acc: 0.007 - ETA: 1:24:07 - loss: 15.9987 - acc: 0.007 - ETA: 1:23:50 - loss: 16.0030 - acc: 0.007 - ETA: 1:23:22 - loss: 16.0069 - acc: 0.006 - ETA: 1:22:52 - loss: 16.0106 - acc: 0.006 - ETA: 1:22:28 - loss: 15.9881 - acc: 0.008 - ETA: 1:21:56 - loss: 15.9922 - acc: 0.007 - ETA: 1:21:25 - loss: 15.9960 - acc: 0.007 - ETA: 1:20:53 - loss: 15.9996 - acc: 0.007 - ETA: 1:20:22 - loss: 15.9799 - acc: 0.008 - ETA: 1:19:51 - loss: 15.9838 - acc: 0.008 - ETA: 1:19:20 - loss: 15.9874 - acc: 0.008 - ETA: 1:18:52 - loss: 15.9908 - acc: 0.007 - ETA: 1:18:21 - loss: 15.9941 - acc: 0.007 - ETA: 1:17:50 - loss: 15.9972 - acc: 0.007 - ETA: 1:17:18 - loss: 16.0002 - acc: 0.007 - ETA: 1:16:50 - loss: 16.0030 - acc: 0.007 - ETA: 1:16:17 - loss: 16.0056 - acc: 0.007 - ETA: 1:15:48 - loss: 16.0082 - acc: 0.006 - ETA: 1:15:14 - loss: 16.0106 - acc: 0.006 - ETA: 1:14:45 - loss: 16.0130 - acc: 0.006 - ETA: 1:14:15 - loss: 16.0152 - acc: 0.006 - ETA: 1:13:43 - loss: 16.0174 - acc: 0.006 - ETA: 1:13:14 - loss: 16.0194 - acc: 0.006 - ETA: 1:12:42 - loss: 16.0214 - acc: 0.006 - ETA: 1:12:12 - loss: 16.0233 - acc: 0.005 - ETA: 1:11:40 - loss: 16.0251 - acc: 0.005 - ETA: 1:11:09 - loss: 16.0269 - acc: 0.005 - ETA: 1:10:38 - loss: 16.0286 - acc: 0.005 - ETA: 1:10:06 - loss: 16.0302 - acc: 0.005 - ETA: 1:09:36 - loss: 16.0317 - acc: 0.005 - ETA: 1:09:05 - loss: 16.0333 - acc: 0.005 - ETA: 1:08:34 - loss: 16.0347 - acc: 0.005 - ETA: 1:08:02 - loss: 16.0225 - acc: 0.005 - ETA: 1:07:32 - loss: 16.0241 - acc: 0.005 - ETA: 1:07:03 - loss: 16.0256 - acc: 0.005 - ETA: 1:06:36 - loss: 16.0271 - acc: 0.005 - ETA: 1:06:05 - loss: 16.0286 - acc: 0.005 - ETA: 1:05:35 - loss: 16.0299 - acc: 0.005 - ETA: 1:05:04 - loss: 16.0313 - acc: 0.005 - ETA: 1:04:33 - loss: 16.0326 - acc: 0.005 - ETA: 1:04:03 - loss: 16.0339 - acc: 0.005 - ETA: 1:03:31 - loss: 16.0351 - acc: 0.005 - ETA: 1:03:02 - loss: 16.0363 - acc: 0.005 - ETA: 1:02:30 - loss: 16.0375 - acc: 0.005 - ETA: 1:02:01 - loss: 16.0273 - acc: 0.005 - ETA: 1:01:31 - loss: 16.0286 - acc: 0.005 - ETA: 1:01:00 - loss: 16.0298 - acc: 0.005 - ETA: 1:00:30 - loss: 16.0310 - acc: 0.005 - ETA: 59:59 - loss: 16.0321 - acc: 0.0053  - ETA: 59:29 - loss: 16.0227 - acc: 0.005 - ETA: 58:58 - loss: 16.0239 - acc: 0.005 - ETA: 58:28 - loss: 16.0251 - acc: 0.005 - ETA: 57:57 - loss: 16.0263 - acc: 0.005 - ETA: 57:27 - loss: 16.0274 - acc: 0.005 - ETA: 56:56 - loss: 16.0286 - acc: 0.005 - ETA: 56:26 - loss: 16.0296 - acc: 0.005 - ETA: 55:57 - loss: 16.0307 - acc: 0.005 - ETA: 55:25 - loss: 16.0317 - acc: 0.005 - ETA: 54:56 - loss: 16.0328 - acc: 0.005 - ETA: 54:25 - loss: 16.0338 - acc: 0.005 - ETA: 53:56 - loss: 16.0347 - acc: 0.005 - ETA: 53:25 - loss: 16.0357 - acc: 0.005 - ETA: 52:55 - loss: 16.0366 - acc: 0.005 - ETA: 52:25 - loss: 16.0375 - acc: 0.005 - ETA: 51:54 - loss: 16.0384 - acc: 0.004 - ETA: 51:24 - loss: 16.0217 - acc: 0.006 - ETA: 50:53 - loss: 16.0141 - acc: 0.006 - ETA: 50:23 - loss: 16.0152 - acc: 0.006 - ETA: 49:53 - loss: 16.0078 - acc: 0.006 - ETA: 49:25 - loss: 16.0090 - acc: 0.006 - ETA: 48:58 - loss: 16.0101 - acc: 0.006 - ETA: 48:29 - loss: 16.0030 - acc: 0.007 - ETA: 47:59 - loss: 16.0041 - acc: 0.007 - ETA: 47:29 - loss: 16.0053 - acc: 0.007 - ETA: 47:01 - loss: 16.0064 - acc: 0.006 - ETA: 46:30 - loss: 16.0075 - acc: 0.006 - ETA: 46:00 - loss: 16.0086 - acc: 0.006 - ETA: 45:30 - loss: 16.0096 - acc: 0.006 - ETA: 45:01 - loss: 16.0106 - acc: 0.006 - ETA: 44:31 - loss: 16.0117 - acc: 0.006 - ETA: 44:02 - loss: 16.0051 - acc: 0.007 - ETA: 43:33 - loss: 16.0062 - acc: 0.006 - ETA: 43:05 - loss: 16.0072 - acc: 0.006 - ETA: 42:34 - loss: 16.0082 - acc: 0.006 - ETA: 42:04 - loss: 16.0092 - acc: 0.006 - ETA: 41:34 - loss: 16.0102 - acc: 0.006 - ETA: 41:06 - loss: 16.0111 - acc: 0.006 - ETA: 40:36 - loss: 16.0121 - acc: 0.006 - ETA: 40:05 - loss: 16.0130 - acc: 0.006 - ETA: 39:35 - loss: 16.0069 - acc: 0.006 - ETA: 39:05 - loss: 16.0079 - acc: 0.006 - ETA: 38:36 - loss: 16.0088 - acc: 0.006 - ETA: 38:07 - loss: 16.0030 - acc: 0.007 - ETA: 37:37 - loss: 16.0039 - acc: 0.007 - ETA: 37:07 - loss: 16.0049 - acc: 0.007 - ETA: 36:36 - loss: 16.0058 - acc: 0.007 - ETA: 36:06 - loss: 16.0002 - acc: 0.007 - ETA: 35:36 - loss: 16.0011 - acc: 0.007 - ETA: 35:06 - loss: 16.0020 - acc: 0.007 - ETA: 34:35 - loss: 16.0030 - acc: 0.007 - ETA: 34:05 - loss: 16.0039 - acc: 0.007 - ETA: 33:35 - loss: 16.0048 - acc: 0.007 - ETA: 33:05 - loss: 15.9994 - acc: 0.007 - ETA: 32:34 - loss: 16.0003 - acc: 0.007 - ETA: 32:04 - loss: 15.9951 - acc: 0.007 - ETA: 31:33 - loss: 15.9960 - acc: 0.007 - ETA: 31:03 - loss: 15.9969 - acc: 0.007 - ETA: 30:33 - loss: 15.9978 - acc: 0.007 - ETA: 30:03 - loss: 15.9987 - acc: 0.007 - ETA: 29:33 - loss: 15.9937 - acc: 0.007 - ETA: 29:02 - loss: 15.9946 - acc: 0.007 - ETA: 28:32 - loss: 15.9955 - acc: 0.007 - ETA: 28:02 - loss: 15.9963 - acc: 0.007 - ETA: 27:32 - loss: 15.9972 - acc: 0.007 - ETA: 27:02 - loss: 15.9981 - acc: 0.007 - ETA: 26:31 - loss: 15.9989 - acc: 0.007 - ETA: 26:01 - loss: 15.9997 - acc: 0.007 - ETA: 25:30 - loss: 16.0006 - acc: 0.007 - ETA: 26:07 - loss: 16.0014 - acc: 0.007 - ETA: 25:41 - loss: 16.0022 - acc: 0.007 - ETA: 25:11 - loss: 15.9975 - acc: 0.007 - ETA: 24:41 - loss: 15.9929 - acc: 0.007 - ETA: 24:11 - loss: 15.9937 - acc: 0.007 - ETA: 23:40 - loss: 15.9945 - acc: 0.007 - ETA: 23:10 - loss: 15.9953 - acc: 0.007 - ETA: 22:41 - loss: 15.9908 - acc: 0.007 - ETA: 22:11 - loss: 15.9917 - acc: 0.0078"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-9b9a72a7c69e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#custom_model.load_weights('saved_models/weights.best.custom.hdf5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#VGG16_model.fit(train_tensors, train_targets, validation_split=0.2,epochs=10, batch_size=20, callbacks=[checkpointer3], verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcustom_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "checkpointer3 = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "checkpointer4 = ModelCheckpoint(filepath='saved_models/weights.best.custom.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "#custom_model.load_weights('saved_models/weights.best.custom.hdf5')\n",
    "#VGG16_model.fit(train_tensors, train_targets, validation_split=0.2,epochs=10, batch_size=20, callbacks=[checkpointer3], verbose=1)\n",
    "custom_model.fit(train_tensors, train_targets, validation_split=0.3,epochs=100, batch_size=20, callbacks=[checkpointer4], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c582671367ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get index of predicted dog breed for each image in test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcustom_model_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_tensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# report test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_model_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_model_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-c582671367ff>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get index of predicted dog breed for each image in test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcustom_model_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_tensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# report test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_model_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_model_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "custom_model_predictions = [np.argmax(custom_model.predict(np.expand_dims(feature, axis=0))) for feature in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(custom_model_predictions)==np.argmax(test_targets, axis=1))/len(custom_model_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Brand with the Model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
